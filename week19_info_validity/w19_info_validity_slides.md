# Week 19: Information Validity Workshop

**Cambridge TECHNICALS Level 3 Engineering**  
**Unit 24: Project Management for Engineers**  
**Learning Outcome: LO4.4**

---

## Table of Contents

| Section | Topic | Time (mins) | LO |
|---------|-------|-------------|-----|
| 1 | Introduction to Information Validity | 5 | LO4.4 |
| 2 | Why Validity Matters | 5 | LO4.4 |
| 3 | Validity Criteria Part 1 | 15 | LO4.4 |
| 4 | Validity Criteria Part 2 | 15 | LO4.4 |
| 5 | Applying Validity Assessment | 10 | LO4.4 |
| 6 | Common Pitfalls and Best Practices | 5 | LO4.4 |
| 7 | Conclusion | 5 | LO4.4 |

<div style="text-align: right; font-size: 0.8em;">Slide 2</div>

---

## Learning Outcomes

By the end of this lecture, you will be able to:

1. Define and explain each of the ten criteria for judging information validity
2. Distinguish between similar validity criteria (complexity vs detail, accuracy vs reliability)
3. Apply validity assessment frameworks to engineering project scenarios
4. Identify common threats to information validity
5. Make justified recommendations about information source suitability

<div style="text-align: right; font-size: 0.8em;">Slide 3</div>

---

## Objectives

This lecture will enable you to:

- Understand systematic frameworks for evaluating information quality
- Apply ten specific validity criteria to project management decisions
- Recognise the difference between high and low validity information sources
- Avoid costly errors caused by inadequate information assessment
- Make evidence-based recommendations about data reliability

<div style="text-align: right; font-size: 0.8em;">Slide 4</div>

---

## Introduction to Information Validity

**The Challenge:**
- Engineering project managers make critical decisions daily
- Decisions affect budgets, schedules, safety, and stakeholder satisfaction
- Not all information possesses equal value for decision-making

**The Risk:**
- Authoritative-looking information may contain hidden biases
- Detailed reports may lack relevance to project context
- Up-to-date sources may still provide unreliable data

<div style="text-align: right; font-size: 0.8em;">Slide 5</div>

---

## The Validity Assessment Framework

**Ten Validity Criteria:**

```
SOURCE CHARACTERISTICS    CONTENT ATTRIBUTES      FITNESS FOR PURPOSE
┌──────────────────┐     ┌─────────────────┐     ┌──────────────┐
│ Integrity        │     │ Relevance       │     │ Importance   │
│ Bias             │     │ Complexity      │     │              │
│                  │     │ Degree of Detail│     │              │
│ Currency         │     │ Quality         │     │              │
│                  │     │ Accuracy        │     │              │
│                  │     │ Reliability     │     │              │
└──────────────────┘     └─────────────────┘     └──────────────┘
```

<div style="text-align: right; font-size: 0.8em;">Slide 6</div>

---

## Why Validity Matters in Engineering Projects

**The Evidence:**
- UK infrastructure projects: 32% average cost overruns
- 67% cited inadequate information assessment as a contributing factor
- £2.3 billion annual project losses across UK engineering sectors

**Three Categories of Validity Failures:**
1. Source-Related Failures
2. Content-Related Failures  
3. Temporal and Contextual Failures

<div style="text-align: right; font-size: 0.8em;">Slide 7</div>

---

## Consequence of Poor Information Assessment

**Real-World Impact:**

| Failure Type | Example | Consequence |
|--------------|---------|-------------|
| Source | Using supplier marketing as sole technical basis | Over-specification, cost overruns |
| Content | Wrong complexity level for audience | Miscommunication, poor decisions |
| Temporal | Outdated industry standards | Non-compliance, rework |

<div style="text-align: right; font-size: 0.8em;">Slide 8</div>

---

## Criterion 1: Integrity of Source

**Definition:** Whether information originates from trustworthy, credible, and appropriately qualified sources

**High-Integrity Sources:**
- Peer-reviewed academic journals
- Professional institution publications (IMechE, ICE)
- Government research bodies and regulatory agencies
- Independent testing laboratories
- Established standards bodies (ISO, BSI)

**Lower-Integrity Sources:**
- Supplier marketing materials
- Unattributed internet sources
- Social media posts
- Single anecdotal evidence

<div style="text-align: right; font-size: 0.8em;">Slide 9</div>

---

## Criterion 2: Bias of Source

**Definition:** Systematic deviation from objectivity caused by source interests, assumptions, or perspectives

**Common Bias Categories:**

```
COMMERCIAL BIAS          CONFIRMATION BIAS        TEMPORAL BIAS
Suppliers emphasize  →   Selective data      →    Historical data
advantages              presentation             may not persist

CULTURAL/ORG BIAS        
Assumptions specific
to particular contexts
```

**Assessment Question:** What interests does the source have in how this information is received?

<div style="text-align: right; font-size: 0.8em;">Slide 10</div>

---

## Criterion 3: Relevance

**Definition:** Whether information applies directly to the specific project context, decision requirement, or problem

**Relevance Assessment Considers:**
- **Contextual Fit:** Does it address the specific engineering domain, technology, scale?
- **Decision Alignment:** Does it help answer the specific questions facing the project manager?
- **Temporal Alignment:** Does it address the current project phase?

**Key Question:** Are we confusing interesting information with useful information?

<div style="text-align: right; font-size: 0.8em;">Slide 11</div>

---

## Criterion 4: Complexity

**Definition:** How difficult information is to understand (technical language, mathematics, assumed knowledge, structure)

**Important Distinction:**
- Complexity = **difficulty of comprehension**
- Degree of detail = quantity of content

**Appropriate Complexity:**
- Matches audience expertise
- Suits communication purpose
- Overly complex → misunderstanding
- Oversimplified → omits critical nuances

<div style="text-align: right; font-size: 0.8em;">Slide 12</div>

---

## Criterion 5: Degree of Detail

**Definition:** The quantity and depth of information provided (specificity, data points, disaggregation, thoroughness)

**Detail Requirements by Purpose:**

| Purpose | Detail Level | Example |
|---------|--------------|---------|
| Problem diagnosis | High | Individual activity completion % |
| Status reporting | Medium | Phase-level completion status |
| Executive briefing | Low | Overall completion % only |

**Remember:** Information may be highly detailed yet presented simply, or contain little detail but be expressed in complex technical language

<div style="text-align: right; font-size: 0.8em;">Slide 13</div>

---

## Criterion 6: Currency

**Definition:** Whether information remains current, up-to-date, and reflects latest knowledge, standards, or conditions

**Currency Requirements Vary:**

```
HIGH CURRENCY                MEDIUM CURRENCY              LOWER CURRENCY
─────────────                ───────────────              ──────────────
Market prices                Technical specs              Fundamental principles
Regulatory requirements      Manufacturing processes      Historical outcomes
Software versions            Material properties          Design methodologies
Economic conditions          Supplier capabilities        Lessons learned
```

<div style="text-align: right; font-size: 0.8em;">Slide 14</div>

---

## Criterion 7: Quality

**Definition:** Whether information meets required standards for its intended purpose

**Quality Indicators:**

1. **Methodology Rigour:** Clear research design, appropriate samples, validated instruments
2. **Review and Verification:** Peer review, editorial oversight, independent validation
3. **Presentation Standards:** Professional formatting, clear structure, proper referencing
4. **Standards Alignment:** Compliance with ISO, BSI, professional institution guidelines

<div style="text-align: right; font-size: 0.8em;">Slide 15</div>

---

## Criterion 8: Accuracy

**Definition:** Whether information correctly represents the reality it purports to describe

**Accuracy Threats:**
- Measurement error (instruments, calibration, interference)
- Data entry and processing errors
- Rounding and approximation issues
- Interpretation and representation mistakes

**Verification Methods:**
- Cross-reference multiple sources
- Request raw data for independent calculation
- Conduct sample verification
- Check internal consistency

<div style="text-align: right; font-size: 0.8em;">Slide 16</div>

---

## Criterion 9: Reliability

**Definition:** Whether information sources produce consistent results when assessed repeatedly or by different evaluators

**Critical Distinction:**

```
ACCURACY vs RELIABILITY
────────    ───────────
Correctly represents    Produces consistent
reality at a            results across
specific time      ≠    multiple assessments
```

**Possible Combinations:**
- High reliability, high accuracy (ideal)
- High reliability, low accuracy (consistently wrong)
- Low reliability, variable accuracy (inconsistent)

<div style="text-align: right; font-size: 0.8em;">Slide 17</div>

---

## Criterion 10: Importance

**Definition:** Whether information addresses matters that significantly affect project outcomes

**Importance Criteria:**
- **Impact Magnitude:** Large financial, schedule, safety, or quality consequences?
- **Risk Exposure:** High-probability, high-impact risks?
- **Decision Criticality:** Irreversible decisions or easily adjusted choices?
- **Stakeholder Significance:** High-power, high-interest stakeholders?

**Resource Allocation:** Evaluation effort should be proportional to information importance

<div style="text-align: right; font-size: 0.8em;">Slide 18</div>

---

## Systematic Validity Assessment Framework

**Structured Evaluation Checklist:**

```
CRITERION            | RATING (H/M/L) | EVIDENCE/NOTES
─────────────────────|────────────────|─────────────────
Integrity of Source  |                |
Bias of Source       |                |
Relevance            |                |
Complexity           |                |
Degree of Detail     |                |
Currency             |                |
Quality              |                |
Accuracy             |                |
Reliability          |                |
Importance           |                |
─────────────────────|────────────────|─────────────────
OVERALL VALIDITY     |                |
RECOMMENDATION       |                |
```

<div style="text-align: right; font-size: 0.8em;">Slide 19</div>

---

## Example: Supplier Selection Scenario

**Context:** Control systems supplier selection for £2.3M automation project

**Three Information Sources:**
1. Supplier's promotional brochure
2. Independent testing laboratory report
3. Colleague's 5-year-old anecdote

**Which source has highest validity for decision-making?**

<div style="text-align: right; font-size: 0.8em;">Slide 20</div>

---

## Validity Assessment Results

| Criterion | Brochure | Lab Report | Anecdote |
|-----------|----------|------------|----------|
| Integrity | Low | High | Medium |
| Bias | High | Low | Medium |
| Relevance | Medium | High | Low |
| Currency | Unknown | High | Low |
| Quality | Medium | High | Low |
| Reliability | Unknown | High | Low |
| **Overall** | **LOW** | **HIGH** | **LOW** |

**Conclusion:** Only lab report provides adequate decision basis

<div style="text-align: right; font-size: 0.8em;">Slide 21</div>

---

## Integration with Project Management Tools

**Validity Assessment Links to:**

| PM Framework | Validity Application |
|--------------|----------------------|
| Risk Management | Low-validity threat identification → inadequate contingency |
| Stakeholder Management | Validity of stakeholder feedback and requirements |
| Quality Control | Measurement system reliability before process adjustments |
| Decision Analysis | Information validity as weighting factor |

<div style="text-align: right; font-size: 0.8em;">Slide 22</div>

---

## Common Validity Assessment Errors

**Five Key Pitfalls:**

1. **Availability Bias:** Overweighting easily accessible information
2. **Confirmation Seeking:** Lenient assessment for information supporting preferred outcomes
3. **Halo Effect:** Assuming prestigious sources automatically have high validity
4. **Complexity Confusion:** Mistaking complex presentation for quality
5. **Single-Criterion Focus:** Assessing one criterion whilst ignoring others

<div style="text-align: right; font-size: 0.8em;">Slide 23</div>

---

## Best Practices for Project Managers

**Six Essential Practices:**

1. **Establish Validation Protocols:** Standard procedures proportional to decision importance
2. **Document Assessment Rationale:** Record reasoning for significant decisions
3. **Seek Multiple Perspectives:** Consult multiple independent sources
4. **Challenge Comfortable Information:** Scrutinise information supporting preferred outcomes
5. **Maintain Uncertainty Awareness:** Express appropriate confidence levels
6. **Update Assessments:** Revisit as projects progress and new information emerges

<div style="text-align: right; font-size: 0.8em;">Slide 24</div>

---

## Practical Application Guidelines

**Decision-Making Framework:**

```
HIGH IMPORTANCE + HIGH STAKES
    ↓
Systematic evaluation using formal checklists
Multiple independent sources
Expert consultation
    ↓
DOCUMENTED ASSESSMENT

LOW IMPORTANCE + ROUTINE DECISIONS
    ↓
Abbreviated assessment
Basic credibility checks
    ↓
MINIMAL DOCUMENTATION
```

<div style="text-align: right; font-size: 0.8em;">Slide 25</div>

---

## Key Distinctions to Remember

**Exam-Tested Concepts:**

| Often Confused | Actual Difference |
|----------------|-------------------|
| Complexity vs Detail | Difficulty to understand vs Quantity of information |
| Accuracy vs Reliability | Correct at one time vs Consistent across assessments |
| Bias vs Low Integrity | Systematic deviation vs Lack of trustworthiness |
| Relevance vs Importance | Applies to context vs Affects outcomes significantly |

<div style="text-align: right; font-size: 0.8em;">Slide 26</div>

---

## When to Reject Information

**RED FLAGS – Information Should Be Rejected When:**

- Multiple validity criteria score LOW
- High-importance decisions rely on low-integrity sources
- Critical accuracy required but verification impossible
- Obvious commercial bias with no independent validation
- Outdated information for rapidly changing domains
- Complexity inappropriate for intended audience

**Action:** Seek alternative sources or delay decision until adequate information available

<div style="text-align: right; font-size: 0.8em;">Slide 27</div>

---

## Developing Validity Assessment Competency

**Building Professional Habits:**

- Make validity assessment automatic in information gathering
- Practice structured evaluation using checklists
- Reflect on past assessments and outcomes
- Learn from cases where inadequate validation caused project difficulties
- Develop professional scepticism balanced with practical decision-making

**Like other PM skills, validity assessment improves through deliberate practice**

<div style="text-align: right; font-size: 0.8em;">Slide 28</div>

---

## Conclusion: Essential Points

**Five Key Takeaways:**

1. **All criteria matter** – effective assessment considers all ten criteria
2. **Context drives assessment** – critical decisions require stringent validation
3. **Distinguish similar concepts** – clarity prevents examination errors
4. **Systematic beats intuitive** – structured assessment outperforms intuition
5. **Document and update** – recording rationale enables quality assurance

**Professional Impact:** Engineers who combine subject expertise with rigorous information assessment contribute significantly to organisational success

<div style="text-align: right; font-size: 0.8em;">Slide 29</div>

---

## Summary: The Validity Assessment Cycle

```
INFORMATION RECEIVED
        ↓
  APPLY 10 CRITERIA
        ↓
  RATE EACH CRITERION
   (High/Medium/Low)
        ↓
  ASSESS OVERALL VALIDITY
        ↓
   DECISION POINT
        ↓
    ┌───┴───┐
ACCEPT    REJECT
  ↓         ↓
USE FOR   SEEK ALTERNATIVES
DECISION  OR DELAY
```

**Next:** Workshop exercises to practice validity assessment

<div style="text-align: right; font-size: 0.8em;">Slide 30</div>